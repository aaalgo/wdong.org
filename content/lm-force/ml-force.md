Title: 机器学习内功总纲
Date: 2014-05-07
Category: misc

```
我觉得机器学习的万法之宗就是奥康姆剃刀: 拟合效果类似，模型越简单预测能力越强
。从不同的对“简单”的定义出发，就产生了不同的流派。比如：

1. 特征维度越低就越简单(参考维度诅咒)。从这一点出发产生了各种降维算法，像PCA
, LDA（有两个完全不同的LDA，但本质上都是降维)等。多层神经网络也可以看成一种
降维算法。Hinton在Science上那片autoencoder的文章标题就是"降维"，降维的重要性
可见一斑。将降维推广其实就是数据压缩。甚至有一种观点认为数据压缩做到了极致就
是人工智能。

2. 特征的非零维度越少就越简单。所谓的Sparse Coding是机器视觉和相关方向非常重
要的研究课题。在2012年neural network异军突起以前，几乎所有的图像识别算法都要
用到某种Sparse Coding。大家知道如果D维空间内的点用包含D个向量的基标出就是一
个D维向量，这D个维几乎不可能为0。实现sparse coding的方法就是两路:1. 允许有误
差。2.增加基的个数。最土的sparse coding就是k-means clustering（也叫vector 
quantization）。更加一般性的做法是在训练模型的时候加一个L1-regularization来
实现稀疏性。这就引出了下一类方法。

3. Regularization。一个向量维数虽然大，但是缩小每个维度的取值范围，也是一种
形式的简单。从整体来看，缩小取值范围的一个一般做法就是优化向量(或者模型)的模
。从统计的观点看，如果假设数据符合正态分布，最大似然估计基本上就等价于L2-
regularization。着一个流派的学习算法往往都是解下面形式的一个优化问题

        min_M  |f(x;M) - y| + a|M|
               训练误差项      regularization

目前解这类问题的主流是下山法(SGD)。因为加regulariztion项形式上看很容易，所以
往往会被滥用。不管前面出现了什么指数函数对数函数，后面统统来一个
regularization。拿搞物理的人的说法是，连量纲都对不上。更别说统计意义了。神奇
的是竟然还都有一定的效果。

4. 减少训练数据摄入。如果特征是抽象空间的点，没有维度的概念，也没有模的概念
，这时候怎么办？有一种办法是选取所有训练数据中最具代表性/最关键的一小撮样本
来产生模型。从这个角度出发就产生了SVM和boosting这类算法。在SVM中，这类关键样
本被称为 support vector。SV的个数其实就是模型的维度。用SGD训练SVM的时候，如
果碰到一个预测正确的样本，就直接跳过, 碰到错误的才更新模型。 从这个角度推广
一下，按拟合程度如何对数据/模型加权重，基本上就得到了boosting。(boosting不是
这么产生的，但不妨这么理解。) 减少训练数据摄入不是指减小原始训练集的大小，而是
对原始训练集进行精简。比如最终都精简到1兆样本，那么增大原始训练集，比如从10兆
到100兆还是会提高预测精度。

这个是我七八年来学习ML的一些心得，希望对新人有所帮助。哪天看到一种效果很好的
新方法，但是想不通为什么这个方法效果好，或者对方法的来龙去脉摸不着头脑的时候
，不妨万法归宗，往奥康目剃刀上扯一扯，或许就融会贯通了。
```
